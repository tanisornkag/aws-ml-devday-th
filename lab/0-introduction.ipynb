{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Building the LLM-powered chatbot \"AWSomeChat\" with retrieval-augmented generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this Lab, we'll explore how to build GenAI-powered applications capable of performing tasks within a specific domain. The application we will be building in a step-by-step process leverages the retrieval-augmented generation (RAG) design pattern and consists of multiple components ranging out of the broad service portfolio of AWS. \n",
    "\n",
    "## Background and Details\n",
    "\n",
    "We have two primary [types of knowledge for LLMs](https://www.pinecone.io/learn/langchain-retrieval-augmentation/): \n",
    "- **Parametric knowledge**: refers to everything the LLM learned during training and acts as a frozen snapshot of the world for the LLM. \n",
    "- **Source knowledge**: covers any information fed into the LLM via the input prompt. \n",
    "\n",
    "\n",
    "### Retrieval-augmented generation (RAG)\n",
    "\n",
    "![rag-concept](../img/rag-concept.png)\n",
    "\n",
    "The design pattern of retrieval-augmented generation is depicted in the above figure. It works as follows:\n",
    "\n",
    "- Step 0: Knowledge documents / document sequences are encoded and ingested into a vector database. \n",
    "- Step 1: Customer e-mail query is pre-processed and/or tokenized\n",
    "- Step 2: Tokenized input query is encoded\n",
    "- Step 3: Encoded query is used to retrieve most similar text passages in document index using vector similarity search (e.g., Mixed Inner Product Search)\n",
    "- Step 4: Top-k retrieved documents/text passages in combination with original customer e-mail query and e-mail generation prompt are fed into Generator model (Encoder-Decoder) to generate response e-mail\n",
    "\n",
    "### Architecture\n",
    "\n",
    "![rag-architecture](../img/rag-architecture.png)\n",
    "\n",
    "Above figure shows the architecture for the LLM-powered chatbot with retrieval-augmented generation component we will be implementing in this lab. It consists of the following components:\n",
    "- Document store & semantic search: We leverage semantic document search service Amazon Kendra as fully managed embeddings/vector store as well as for a fully managed solution for document retrieval based on questions/asks in natural language.\n",
    "- Response generation: For the chatbot response generation, we use the open-source encoder-decoder model FLAN-T5-XXL conveniently deployed in a one-click fashion through Amazon SageMaker JumpStart right into your VPC.\n",
    "- Orchestration layer: For hosting the orchestration layer implmented with the popular framework langchain we choose a serverless approach. The orchestration layer is exposed as RESTful API service via a Amazon API Gateway.\n",
    "- Conversational Memory: In order to be able to keep track of different chatbot conversation turns while keeping the orchestration layer stateless we integrate the chatbot's memory with Amazon DynamoDB as a storage component.\n",
    "- Frontend: The chatbot frontend is a web application hosted in a Docker container on Amazon ECS. For storing the container image we leverage Amazon ECR. The website is exposed through an Amazon Elastic Load Balancer. \n",
    "\n",
    "## Instructions\n",
    "\n",
    "### Prerequisites\n",
    "#### Recommended background\n",
    "It will be easier for you to run this workshop if you have:\n",
    "\n",
    "- Experience with Deep learning models\n",
    "- Familiarity with Python or other similar programming languages\n",
    "- Experience with Jupyter notebooks\n",
    "- Beginners level knowledge and experience with SageMaker Hosting/Inference.\n",
    "- Beginners level knowledge and experience with Large Language Models\n",
    "\n",
    "#### Target audience\n",
    "Data Scientists, ML Engineering, ML Infrastructure, MLOps Engineers, Technical Leaders.\n",
    "Intended for customers working with large Generative AI models including Language, Computer vision and Multi-modal use-cases.\n",
    "Customers using EKS/EC2/ECS/On-prem for hosting or experience with SageMaker.\n",
    "\n",
    "Level of expertise - 400\n",
    "\n",
    "#### Time to complete\n",
    "Approximately 1 hour."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
